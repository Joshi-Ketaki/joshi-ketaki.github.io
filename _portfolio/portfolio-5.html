---
title: "Distill: Domain-Specific Compilation for Cognitive Models"
excerpt: "<br/><img src='/images/Clone_Detection_Common_Operations.png'> 
<br/>
Cognitive models are time-consuming to create because they require the combination of numerous types of computational tasks. They perform poorly because they are 
typically written in high-level languages such as Python. In this work, we present Distill, a domain-specific compilation tool for accelerating cognitive models 
while still allowing cognitive scientists to develop models in flexible high-level languages."
collection: portfolio
---

Computational models of cognition enable a better understanding of the human brain and behavior, psychiatric and
neurological illnesses, clinical interventions to treat illnesses. They offer a path towards human-like artificial intelligence.
Cognitive models are laborious to develop, requiring the composition of many types of computational tasks. They suffer
from poor performance as they are generally designed using high-level languages like Python. In this work, we present Distill,
a domain-specific compilation tool to accelerate cognitive models while continuing to offer cognitive scientists the ability to 
develop their models in flexible high-level languages. 

<br/><img src='/images/Normalized_Execution_Time_Distil.png'> 
<br/>
Distill uses domain-specific knowledge to compile Python-based cognitive models into LLVM IR, carefully stripping away features like 
dynamic typing and memory management that add performance overheads without being necessary for the underlying computation 
of the models. The net effect is an average of 27× performance improvement in model execution over state-of-the-art 
techniques using Pyston and PyPy. 

<br/><img src='/images/Clone_Detection_Common_Operations.png'> 
<pre>   <b> Identifying similar or overlapping computations in different cognitive models. </b></pre>
<br/>
Distill also repurposes classical compiler data flow analyses to reveal properties of data 
flow in cognitive models that are useful to cognitive scientists such as detecting computational clones to enable reusability of models. 
Distill identifies computational clones in different cognitive models to enable reuse or substitution of complex models by simpler ones.

Distill is publicly  available, integrated with the <a href = "http://www.psyneuln.deptcpanel.princeton.edu/"> PsyNeuLink </a> cognitive modeling environment, and is already being used by 
researchers in the brain sciences.



<br/>
Paper:
<br/>
Ján Veselý, Raghavendra Pradyumna Pothukuchi, <strong>Ketaki Joshi</strong>, Samyak Gupta, Jonathan D. Cohen, and Abhishek Bhattacharjee. 2022. Distill: domain-specific compilation for cognitive models. In Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization (CGO '22). IEEE Press, 301–312. https://doi.org/10.1109/CGO53902.2022.9741278
